```latex
@article{Santoro2021SymbolicBI, title={Symbolic Behaviour in Artificial Intelligence}, author={Adam Santoro and Andrew Kyle Lampinen and Kory Wallace Mathewson and Timothy P. Lillicrap and David Raposo}, journal={ArXiv}, year={2021}, volume={abs/2102.03406} }
```
# Symbolic Behaviour in Artificial Intelligence
Santoro et al. argue that symbol use is evidenced by a set of behaviors, including agreeing on symbol meaning by their users, and that therefore fluent symbol use can only emerge in environments that require coordination around symbol meaning.
#neurosymbolic #deepmind

# Summary
## Symbolic Behavior
### Receptive
The ability to incorporate new symbols.

### Constructive
The ability to construct symbols on a given substrate.

### Embedded
Symbols are interconnected in a symbolic system.

### Malleable
The ability to modify the meaning of symbols. 

### Meaningful
A user's interaction with a symbol should
    1. communicate their meaning,
    2. and _the reasoning process_ that produced their meaning.

### Graded
Each of the preceding criteria exist on a spectrum.

# Connections
Because [[AI is Search]] 
    1. **throughput is the most important metric**, 
    2. compression enables greater throughput, 
    3. **symbols are compression**.
    4. [[Intelligence is Compression]]  

# Key References
* Deep learning for symbolic mathematics
* Deep learning: A critical appraisal
* The next decade in ai: four steps towards robust artificial intelligence
* Building machines that learn and think like people
* Neurosymbolic AI: The 3rd wave
* Reconciling deep learning with symbolic artificial intelligence: representing objects and relations
* Convention: A philosophical study
* The symbolic species: The co-evolution of language and the brain
* Imitating interactive intelligence
* BERT: Pre-training of deep bidirectional transformers for language understanding
* Environmental drivers of systematicity and generalization in a situated agent
* Placing language in an integrated understanding system: Next steps toward human-level performance
* [[Efficient estimation of word representations in vector space]]
* Learning to summarize from human feedback
* The neural architecture of language: Integrative reverse-engineering converges on a model for predictive processing
* [[Implicit representations of meaning in neural language models]]
* Metaphors we live by
* Lime: Learning inductive bias for primitives of mathematical reasoning
* Scaling laws for neural language models
* PaLM: Scaling Language Modeling with Pathways